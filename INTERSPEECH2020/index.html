<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>
Speech2Face: Learning the Face Behind a Voice
</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-65563403-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-65563403-4');
</script>
</head>
<body>
<div class="container">
  <p>&nbsp;</p>
  <p><span class="venue">Conference</span></p>
  <p><span class="title">Title</span></p>
  <br />
  <table border="0" align="center" class="authors">
    <tr align="center" valign="bottom">
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/qu.html" target="_blank">Leyuan Qu</a><sup>&#10013;</sup></td>
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" target="_blank">Cornelius Weber</a></td>
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" target="_blank">Stefan Wermter</a></td>
    </tr>
  </table>
  <br />
  <table border="0" align="center" class="affiliations">
    <tr align="center" valign="middle">
      <td width="20%" align="center" style="padding:0 0px 0 0px;"><img src="images/logo/uhh-and-wtm.png" width="500" height="300" alt=""/></td>
  <td width="97" align="center" style="padding:0 0px 0 0px;"><a href="https://www.csail.mit.edu/">WTM</a></td>
    </tr>
  </table>
  <br />
  <br />
  <br />
  <br />
  <br />
  <br />

  <p><span class="section">Demos</span> </p>
  <table width="100%" border="0" align="center">
    <tr>
      <th width="20%" align="center" rowspan="2">Video</th>
      <th width="20%" class="tg-0lax" rowspan="6"></th>
      <th class="tg-c3ow"></th>
      <th width="20%" align="center">Mixed wav</th>
      <th width="20%" align="center">Target wav</th>
    </tr>
    <tr>
      <td class="tg-c3ow"></td>
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <td align="center" rowspan="4">
        <video width="280" height="280" controls>
          <source src="videos/00005.mp4.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td class="tg-c3ow"></td>
      <th width="20%" align="center"><br>Reference embedding</th>
      <th width="20%" align="center"><br>Separated wav</th>
    </tr>
    <tr>
      <th width="40%" align="center">Voice</th>
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <th width="40%" align="center">Face</th>
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="100" /></td>
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <th width="40%" align="center">Voice + Face</th>
      <!-- <th width="20%" align="center"></th> -->
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="100" />  <audio controls="controls" style="width: 110px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <!-- <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td> -->
      <td align="center"><audio controls class="audio-player" preload="metadata">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
  </table>

  <br>
  <br>
  <br>
  <br>
  <p><span class="section">More samples</span> </p>
  <table width="100%" border="0" align="center">
    <tr>
      <th align="center">Mixed wav</th>
      <th align="center">Target wav</th>
      <th align="center">Reference</th>
      <th align="center">Separated wav</th>
    </tr>
    <tr>
      <td class="tg-0lax"></td>
      <td class="tg-0lax"></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="80" /></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <td class="tg-0lax"></td>
      <td class="tg-0lax"></td>
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="80" />  <audio controls="controls" style="width: 110px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
  </table>



  <br>
  <br>
  <table width="100%" border="0" align="center">
    <tr>
      <td class="tg-0lax"></td>
      <td class="tg-0lax"></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="80" /></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
    <tr>
      <td class="tg-0lax"></td>
      <td class="tg-0lax"></td>
      <td align="center" width="16%"><img src="images/teaser/orig/1044Phyllis_Diller_qoc0k5YwbRI_0000019.jpg" width="80" />  <audio controls="controls" style="width: 110px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
      <td align="center"><audio controls class="audio-player" preload="metadata" style="width: 200px;">
              <source src="images/teaser/wav/0024Daniel_Craig_L1Ltxf1NeL8_0000007.wav" type="audio/wav" />
              Does not support </audio></td>
    </tr>
  </table>






  <p><span class="section">Abstract</span> </p>
  <p>How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural videos of people speaking from 
Internet/Youtube. During training, our model learns audiovisual, voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. 
Our reconstructions, obtained directly from audio, reveal the  correlations between faces and voices. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions from audio resemble the true face images of the speakers.<br />
    &nbsp;<br />
</p>
  <p class="section">Paper</p>
  <table border="0">
    <tbody>
      <tr>
        <td height="195"><a href="https://arxiv.org/abs/1905.09773"><img src="images/webthumbnail.jpg" alt="" width="175" height="211"/></a></td>
        <td>&nbsp;</td>
  <td>&nbsp;</td>
        <td>&quot;Title&quot;,<br/>
            Tae-Hyun Oh, Tali Dekel, Changil Kim, Inbar Mosseri, William T. Freeman, Michael Rubinstein, Wojciech Matusik<br/>
  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019<br/>
        <p><a href="https://arxiv.org/abs/1905.09773">[Arxiv]</a></p></td>
      </tr>
    </tbody>
  </table>
&nbsp;<br />
  <p class="section">Supplementary Material</p>
  <table width="283" height="136" border="0">
    <tbody>
      <tr>
        <td width="165"><a href="supplemental/index.html"><img src="images/supp_fig.jpg" alt="" width="207"/></a></td>
        <td width="6">&nbsp;</td>
        <td width="56"><p>[<a href="supplemental/index.html">Link</a>]</p></td>
      </tr>
    </tbody>
  </table>
&nbsp;<br />
<p class="section">Ethical Considerations</p>
  <table border="0">
    <tbody>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>
    Although this is a purely academic investigation, we feel that it is important to explicitly discuss in the paper a set of ethical considerations due to the potential sensitivity of facial information.<br/>
    &nbsp;<br/>
  </td>
  </tr>
  <tr>
  <td>&nbsp;</td>
        <td>&nbsp;</td>
  <td>
    <strong>Privacy.</strong> As mentioned, our method cannot recover the true identity of a person from their voice (i.e., an exact image of their face). 
This is because our model is trained to capture visual features (related to age, gender, etc.) that are common to <i>many</i> individuals, and only in cases where there is strong enough evidence to connect those visual features with vocal/speech attributes in the data (see ``voice-face correlations'' below). As such, the model will only produce average-looking faces, with characteristic visual features that are correlated with the input speech. It will not produce images of specific individuals. <br/>
    &nbsp;<br/>
  </td></tr>
  <tr>
  <td>&nbsp;</td>
        <td>&nbsp;</td>
  <td>
    <strong>Voice-face correlations and dataset bias.</strong> Our model is designed to reveal statistical correlations that exist between facial features and voices of speakers in the training data. The training data we use is a collection of educational videos from YouTube, and does not represent equally the entire world population. Therefore, the model---as is the case with any machine learning model---is affected by this uneven distribution of data. <br/>

    More specifically, if a set of speakers might have vocal-visual traits that are relatively uncommon in the data, then the quality of our reconstructions for such cases may degrade.  For example, if a certain language does not appear in the training data, our reconstructions will not capture well the facial attributes that may be correlated with that language. <br/>

    Note that some of the features in our predicted faces may not even be physically connected to speech, for example hair color or style. However, if many speakers in the training set who speak in a similar way (e.g., in the same language) also share some common visual traits (e.g., a common hair color or style), then those visual traits may show up in the predictions. <br/>
 
    For the above reasons, we recommend that any further investigation or practical use of this technology will be carefully tested to ensure that the training data is representative of the intended user population. If that is not the case, more representative data should be broadly collected. <br/>
    &nbsp;<br/>
  </td>
  </tr>
  <tr>
  <td>&nbsp;</td>
        <td>&nbsp;</td>
  <td>
    <strong>Categories.</strong>
    In our experimental section, we mention inferred demographic categories such as "White" and "Asian". These are categories defined and used by a commercial face attribute classifier (<a href="https://www.faceplusplus.com/">Face++</a>), and were only used for evaluation in this paper. Our model is not supplied with and does not make use of this information at any stage.<br/>
    &nbsp;<br/>
  </td>
      </tr>
    </tbody>
  </table>
<p class="section">Further Reading
</p>


<table border="0">
  <tbody>
    <tr>
      <td width="6">&nbsp;</td>
      <td width="6">&nbsp;</td>
      <td width="977"><p><em>&quot;Seeing voices and hearing faces: Cross-modal biometric matching&quot;,  A. Nagrani, S. Albanie, and A. Zisserman, CVPR 2018</em></p>
  <p><em>&quot;On Learning Associations of Faces and Voices&quot;, C. Kim, H. V. Shin, T.-H. Oh, A. Kaspar, M. Elgharib, and W. Matusik, ACCV 2018 </em></p>
        <p><em>&quot;Wav2Pix: speech-conditioned face generation using generative adversarial networks&quot;, A. Duarte, F. Roldan, M. Tubau, J. Escur, S. Pascual, A. Salvador, E. Mohedano, K. McGuinness, J. Torres, and X. Giroi-Nieto, ICASSP 2019 </em></p>
        <p><em>&quot;Disjoint mapping network for cross-modal matching of voices and faces&quot;, Y. Wen, M. A. Ismail, W. Liu, B. Raj, and R. Singh, ICLR 2019</em></p>
        <p><em>&quot;Putting the face to the voice: Matching identity across modality&quot;, M. Kamachi, H. Hill, K. Lander, and E. Vatikiotis-Bateson, Current Biology, 2003        </em><em><br/>
        </em></p></td>
    </tr>
  </tbody>
</table>
<p class="section">Acknowledgment</p>
  <table border="0">
    <tbody>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>
    The authors would like to thank Suwon
Shon, James Glass, Forrester Cole and Dilip Krishnan for
helpful discussion. T.-H. Oh and C. Kim were supported by
QCRI-CSAIL Computer Science Research Program at MIT.
<br/>
    &nbsp;<br/>
  </td>
  </tr>
  </tbody>
  </table>
  <p>&nbsp;</p>
  <p class="section">&nbsp;</p>
  <!--
  <p class="section">Google Research Blog</p>
  <table width="1300" border="0">
    <tbody>
      <tr>
        <td width="136"><img src="images/research_blog.png" width="200" height="131" alt=""/></td>
        <td width="1048"><a href="https://research.googleblog.com/2017/08/making-visible-watermarks-more-effective.html"><img src="images/blog_post.png" width="300" height="166" alt=""/></a></td>
      </tr>
    </tbody>
  </table>
  <p class="section">Press</p>
  <table border="0" cellpadding="10">
    <tbody>
      <tr>
        <td><a href="https://www.theverge.com/2017/8/18/16162108/google-research-algorithm-watermark-removal-photo-protection"><img src="images/the_verge_2016_logo.png" width="200" height="37" alt=""/></a></td>
        <td><a href="https://petapixel.com/2017/08/18/ai-can-easily-erase-photo-watermarks-heres-protect/"><img src="images/petapixel.png" width="200" height="50" alt=""/></a></td>
        <td><a href="https://www.engadget.com/2017/08/18/google-flawlessly-remove-stock-photo-watermarks/"><img src="images/engadget.png" width="200" height="44" alt=""/></a></td>
        <td><a href="https://www.wired.com/story/stock-photo-google-algorithm/"><img src="images/wired-logo.jpg" width="200" height="46" alt=""/></a></td>
      </tr>
      <tr>
        <td><a href="http://www.dailymail.co.uk/sciencetech/article-4803562/Google-AI-easily-erase-watermarks-photos.html"><img src="images/dm_com_29.png" width="210" height="62" alt=""/></a></td>
        <td><a href="https://thenextweb.com/google/2017/08/18/google-watermark-stock-photo-remove/"><img src="images/the_next_web_logo.jpg" width="190" height="100" alt=""/></a></td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </tbody>
  </table>
  <p class="section">&nbsp;</p>
  <p class="section">&nbsp;</p>
  <p class="section">&nbsp;</p>
-->
  <p class="section">&nbsp;</p>
</div>
</body>
</html>
