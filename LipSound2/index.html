<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>
INTERSPEECH2020
</title>
<link href="css/style.css" rel="stylesheet" type="text/css" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-65563403-3"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-65563403-4');
</script>
</head>
<body>
<div class="container">
  <p>&nbsp;</p>
  <p><span class="venue">SUBMITTED TO IEEE TRANSACTION ON NEURAL NETWORKS AND LEARNING SYSTEMS</span></p>
   <br />
    <br />

  <p><span class="title">LipSound2: Self-Supervised Pre-Training <br /> <br />for Lip-to-Speech Reconstruction and Lip Reading</span></p>
  <br />
  <table border="0" align="center" class="authors">
    <tr align="center" valign="bottom">
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/qu.html" target="_blank">Leyuan Qu</a><sup>&#10013;</sup></td>
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/weber.html" target="_blank">Cornelius Weber</a></td>
    <td><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm/people/wermter.html" target="_blank">Stefan Wermter</a></td>
    </tr>
  </table>
  <br />

  <table width="100%" border="0" align="center">
  <tr>
    <td width="97" align="center" style="padding:0 0px 0 0px;"><a href="https://www.inf.uni-hamburg.de/en/inst/ab/wtm.html" target="_blank">Knowledge Technology Group (WTM)</a></td>
  </tr>
  <th width="40%" align="center">Department of Informatics, University of Hamburg</th>
  </table>


  <table width="100%" border="0" align="center">
  <tr>
    <td align="center" width="100%"><img src="images/logo/uhh-and-wtm.png" height="250" width="500" /></td>
  </tr>
  </table>

  <p><span class="section">Abstract</span> </p>
  <p style="font-size:17px;">The aim of this work is to investigate the impact of crossmodal self-supervised pre-training for speech reconstruction (video-to-audio) by leveraging the natural co-occurrence of audio and visual streams in videos. We propose LipSound2 which consists of an encoder-decoder architecture and location-aware attention mechanism to learn to map face image sequences to mel-scale spectrograms directly without requiring any human annotations. The proposed LipSound2 model is firstly pre-trained on ~2400h multi-lingual (e.g. English and German) audio-visual data (VoxCeleb2). To verify the generalizability of the proposed method, we then fine-tune the pre-trained model on specific-domain datasets (GRID, TCD-TIMIT) for English speech reconstruction and achieve significant improvement on speech quality and intelligibility than previous works in speaker-dependent and -independent settings. In addition to English, we conduct Chinese speech reconstruction on the CMLR dataset to verify the impact on transferability. Lastly, we perform the cascaded lip reading (video-to-text) system by fine-tuning the generated audios on a pre-trained speech recognition system and achieve state-of-the-art performance on both English and Chinese benchmark datasets.<br />
    &nbsp;<br />
  </p>
  <br />
  <p><span class="section">Pipeline</span> </p>
  <a><center><img src="images/pipeline.JPG" width="50%" id="architecture"/></center></a>
  <br />

  <p><span class="section">Architecture</span> </p>
  <a><img src="images/architecture.JPG" width="100%" id="architecture"/></a>

  <br />
  <br />
  <br />
  <br />
  <br />
  <br />
  <br />
  <p><span class="section">Demo Video</span> </p>

  <div class="youtube">
	<iframe width="100%"  height="615" class="elementor-video-iframe" src="https://www.youtube.com/embed/clD7-hj6xAU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div> 

  <br>
  <br>
  <br>
  <br>
  <br>
  <br>



<p class="section">Acknowledgment</p>

  <table border="0">
    <tbody>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td  style="font-size:17px;">
        The authors gratefully acknowledge the partial support from the China Scholarship Council (CSC), the German Research Foundation DFG under project CML (TRR 169), and the European Union under project SECURE (No 642667).

<br/>
    &nbsp;<br/>
  </td>

  </tr>
  </tbody>
  </table>
  <p>&nbsp;</p>
  <p class="section">&nbsp;</p>
  <p class="section">&nbsp;</p>
</div>
</body>
</html>
